{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Surah</th>\n",
       "      <th>Ayat</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Translation - Muhammad Tahir-ul-Qadri</th>\n",
       "      <th>Translation - Arthur J</th>\n",
       "      <th>Translation - Marmaduke Pickthall</th>\n",
       "      <th>Tafaseer - Tafsir al-Jalalayn</th>\n",
       "      <th>Tafaseer - Tanwir al-Miqbas min Tafsir Ibn Abbas</th>\n",
       "      <th>EnglishTitle</th>\n",
       "      <th>ArabicTitle</th>\n",
       "      <th>RomanTitle</th>\n",
       "      <th>NumberOfVerses</th>\n",
       "      <th>NumberOfRukus</th>\n",
       "      <th>PlaceOfRevelation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>بِسمِ ٱلله الرَّحْمٰنِ الرَّحِيـمِ</td>\n",
       "      <td>All praise be to Allah alone, the Sustainer of...</td>\n",
       "      <td>In the Name of God, the Merciful, the Compassi...</td>\n",
       "      <td>In the name of Allah, the Beneficent, the Merc...</td>\n",
       "      <td>In the Name of God the Compassionate the Merciful</td>\n",
       "      <td>In the name of Allah, the Beneficent, the Merc...</td>\n",
       "      <td>Al-Fatihah</td>\n",
       "      <td>ٱلْفَاتِحَة</td>\n",
       "      <td>al-Ḥamd</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Makkah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ٱلْحَمْدُ للَّهِ رَبِّ ٱلْعَالَمِينَ</td>\n",
       "      <td>Most Compassionate, Ever-Merciful,</td>\n",
       "      <td>Praise belongs to God, the Lord of all Being,</td>\n",
       "      <td>Praise be to Allah, Lord of the Worlds,</td>\n",
       "      <td>In the Name of God the name of a thing is that...</td>\n",
       "      <td>And on his authority it is related that Ibn 'A...</td>\n",
       "      <td>Al-Fatihah</td>\n",
       "      <td>ٱلْفَاتِحَة</td>\n",
       "      <td>al-Ḥamd</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Makkah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>ٱلرَّحْمـٰنِ ٱلرَّحِيمِ</td>\n",
       "      <td>Master of the Day of Judgment.</td>\n",
       "      <td>the All-merciful, the All-compassionate,</td>\n",
       "      <td>The Beneficent, the Merciful.</td>\n",
       "      <td>The Compassionate the Merciful that is to say ...</td>\n",
       "      <td>(The Beneficent) the Gentle. (The Merciful) th...</td>\n",
       "      <td>Al-Fatihah</td>\n",
       "      <td>ٱلْفَاتِحَة</td>\n",
       "      <td>al-Ḥamd</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Makkah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>مَـٰلِكِ يَوْمِ ٱلدِّينِ</td>\n",
       "      <td>(O Allah!) You alone do we worship and to You ...</td>\n",
       "      <td>the Master of the Day of Doom.</td>\n",
       "      <td>Owner of the Day of Judgment,</td>\n",
       "      <td>Master of the Day of Judgement that is the day...</td>\n",
       "      <td>(Owner of the Day of Judgement) the Arbitrator...</td>\n",
       "      <td>Al-Fatihah</td>\n",
       "      <td>ٱلْفَاتِحَة</td>\n",
       "      <td>al-Ḥamd</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Makkah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>إِيَّاكَ نَعْبُدُ وَإِيَّاكَ نَسْتَعِينُ</td>\n",
       "      <td>Show us the straight path,</td>\n",
       "      <td>Thee only we serve; to Thee alone we pray for ...</td>\n",
       "      <td>Thee (alone) we worship; Thee (alone) we ask f...</td>\n",
       "      <td>You alone we worship and You alone we ask for ...</td>\n",
       "      <td>(Thee (alone) we worship), we turn to you as t...</td>\n",
       "      <td>Al-Fatihah</td>\n",
       "      <td>ٱلْفَاتِحَة</td>\n",
       "      <td>al-Ḥamd</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Makkah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name  Surah  Ayat                                    Arabic  \\\n",
       "0  The Opening      1     1        بِسمِ ٱلله الرَّحْمٰنِ الرَّحِيـمِ   \n",
       "1  The Opening      1     2      ٱلْحَمْدُ للَّهِ رَبِّ ٱلْعَالَمِينَ   \n",
       "2  The Opening      1     3                   ٱلرَّحْمـٰنِ ٱلرَّحِيمِ   \n",
       "3  The Opening      1     4                  مَـٰلِكِ يَوْمِ ٱلدِّينِ   \n",
       "4  The Opening      1     5  إِيَّاكَ نَعْبُدُ وَإِيَّاكَ نَسْتَعِينُ   \n",
       "\n",
       "               Translation - Muhammad Tahir-ul-Qadri  \\\n",
       "0  All praise be to Allah alone, the Sustainer of...   \n",
       "1                 Most Compassionate, Ever-Merciful,   \n",
       "2                     Master of the Day of Judgment.   \n",
       "3  (O Allah!) You alone do we worship and to You ...   \n",
       "4                         Show us the straight path,   \n",
       "\n",
       "                              Translation - Arthur J  \\\n",
       "0  In the Name of God, the Merciful, the Compassi...   \n",
       "1      Praise belongs to God, the Lord of all Being,   \n",
       "2           the All-merciful, the All-compassionate,   \n",
       "3                     the Master of the Day of Doom.   \n",
       "4  Thee only we serve; to Thee alone we pray for ...   \n",
       "\n",
       "                   Translation - Marmaduke Pickthall  \\\n",
       "0  In the name of Allah, the Beneficent, the Merc...   \n",
       "1            Praise be to Allah, Lord of the Worlds,   \n",
       "2                      The Beneficent, the Merciful.   \n",
       "3                      Owner of the Day of Judgment,   \n",
       "4  Thee (alone) we worship; Thee (alone) we ask f...   \n",
       "\n",
       "                       Tafaseer - Tafsir al-Jalalayn  \\\n",
       "0  In the Name of God the Compassionate the Merciful   \n",
       "1  In the Name of God the name of a thing is that...   \n",
       "2  The Compassionate the Merciful that is to say ...   \n",
       "3  Master of the Day of Judgement that is the day...   \n",
       "4  You alone we worship and You alone we ask for ...   \n",
       "\n",
       "    Tafaseer - Tanwir al-Miqbas min Tafsir Ibn Abbas EnglishTitle  \\\n",
       "0  In the name of Allah, the Beneficent, the Merc...   Al-Fatihah   \n",
       "1  And on his authority it is related that Ibn 'A...   Al-Fatihah   \n",
       "2  (The Beneficent) the Gentle. (The Merciful) th...   Al-Fatihah   \n",
       "3  (Owner of the Day of Judgement) the Arbitrator...   Al-Fatihah   \n",
       "4  (Thee (alone) we worship), we turn to you as t...   Al-Fatihah   \n",
       "\n",
       "   ArabicTitle RomanTitle  NumberOfVerses NumberOfRukus PlaceOfRevelation  \n",
       "0  ٱلْفَاتِحَة    al-Ḥamd               7             1            Makkah  \n",
       "1  ٱلْفَاتِحَة    al-Ḥamd               7             1            Makkah  \n",
       "2  ٱلْفَاتِحَة    al-Ḥamd               7             1            Makkah  \n",
       "3  ٱلْفَاتِحَة    al-Ḥamd               7             1            Makkah  \n",
       "4  ٱلْفَاتِحَة    al-Ḥamd               7             1            Makkah  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/main_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_col = list(df.columns)\n",
    "translation_col = [x for x in translation_col if \"Translation\" in x]\n",
    "\n",
    "tafaseer_col = list(df.columns)\n",
    "tafaseer_col = [x for x in tafaseer_col if \"Tafaseer\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic = []\n",
    "text=[]\n",
    "for index, row in df.iterrows():\n",
    "    arabic.append(row['Arabic'])\n",
    "    t = \"\"\n",
    "    t += row['Name'] + \" | \" + str(row['Arabic'])+\" | \"+ str(row['Surah'])+\" | \"+str(row['Ayat'])+\" | \"\n",
    "    t += row['EnglishTitle'] + \" | \" + str(row['ArabicTitle'])+\" | \"+ str(row['RomanTitle'])+\" | \"\n",
    "    t += row['PlaceOfRevelation'] + \" | \"\n",
    "    for j in translation_col:\n",
    "        t += row[j] + \" + \"\n",
    "    t += \" | \"\n",
    "    for j in tafaseer_col:\n",
    "        t+= row[j] + \" + \"\n",
    "    t = t[:-3]\n",
    "    text.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Opening | بِسمِ ٱلله الرَّحْمٰنِ الرَّحِيـمِ | 1 | 1 | Al-Fatihah | ٱلْفَاتِحَة | al-Ḥamd | Makkah | All praise be to Allah alone, the Sustainer of all the worlds, + In the Name of God, the Merciful, the Compassionate + In the name of Allah, the Beneficent, the Merciful. +  | In the Name of God the Compassionate the Merciful + In the name of Allah, the Beneficent, the Merciful.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'بِسمِ ٱلله الرَّحْمٰنِ الرَّحِيـمِ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Preprocess the dataset\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "# Define the search function\n",
    "def search(query, top_k=5):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    sim_scores = cosine_similarity(query_vec, X).flatten()\n",
    "    top_indices = sim_scores.argsort()[::-1][:top_k]\n",
    "    top_paragraphs = [text[i] for i in top_indices]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return top_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = search(\"inheritance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Smoke',\n",
       " 'كَذَلِكَ وَأَوْرَثْنَاهَا قَوْماً آخَرِينَ',\n",
       " '44',\n",
       " '28',\n",
       " 'Ad-Dukhaan',\n",
       " 'ٱلدُّخَان',\n",
       " 'ad-Dukhān',\n",
       " 'Makkah',\n",
       " 'So it happened! We made another people the inheritors of all that. + Even so; and We bequeathed them upon another people. + Even so (it was), and We made it an inheritance for other folk; + ',\n",
       " 'So it was kadhālika is the predicate of a subject that is missing such as ‘the matter was so’ and We made these an inheritance that is to say their riches for another people namely the Children of Israel. + (Even so (it was)) We destroyed them, (and We made it an inheritance for other folk) We made the Children of Israel inherit all that after they were destroyed;']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].split(\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So it was kadhālika is the predicate of a subject that is missing such as ‘the matter was so’ and We made these an inheritance that is to say their riches for another people namely the Children of Israel.',\n",
       " '(Even so (it was)) We destroyed them, (and We made it an inheritance for other folk) We made the Children of Israel inherit all that after they were destroyed;']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].split(\" | \")[-1].split(\" + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So it happened! We made another people the inheritors of all that.',\n",
       " 'Even so; and We bequeathed them upon another people.',\n",
       " 'Even so (it was), and We made it an inheritance for other folk;',\n",
       " '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].split(\" | \")[-2].split(\" + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self, documents):\n",
    "        df = pd.read_csv('data/main_df.csv')\n",
    "        arabic = []\n",
    "        self.documents=[]\n",
    "        translation_col = list(df.columns)\n",
    "        self.translation_col = [x for x in translation_col if \"Translation\" in x]\n",
    "\n",
    "        tafaseer_col = list(df.columns)\n",
    "        self.tafaseer_col = [x for x in tafaseer_col if \"Tafaseer\" in x]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            arabic.append(row['Arabic'])\n",
    "            t = \"\"\n",
    "            t += row['Name'] + \" | \" + str(row['Arabic'])+\" | \"+ str(row['Surah'])+\" | \"+str(row['Ayat'])+\" | \"\n",
    "            t += row['EnglishTitle'] + \" | \" + str(row['ArabicTitle'])+\" | \"+ str(row['RomanTitle'])+\" | \"\n",
    "            t += row['PlaceOfRevelation'] + \" | \"\n",
    "            for j in self.translation_col:\n",
    "                t += row[j] + \" + \"\n",
    "            t += \" | \"\n",
    "            for j in self.tafaseer_col:\n",
    "                t+= row[j] + \" + \"\n",
    "            t = t[:-3]\n",
    "            self.documents.append(t)\n",
    "        \n",
    "        self.total_documents = len(self.documents)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.index = defaultdict(list)\n",
    "        self.document_lengths = []\n",
    "        self.idf = {}\n",
    "\n",
    "        # Preprocess the documents and build the search index\n",
    "        self.build_index()\n",
    "\n",
    "    def preprocess(self, document):\n",
    "        \"\"\"\n",
    "        Tokenize, remove stop words, and stem the words in the document.\n",
    "        \"\"\"\n",
    "        words = word_tokenize(document.lower())\n",
    "        words = [word for word in words if word.isalpha() and word not in self.stop_words]\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(word) for word in words]\n",
    "        return words\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"\n",
    "        Build an inverted index of the preprocessed documents.\n",
    "        \"\"\"\n",
    "        for i, document in enumerate(self.documents):\n",
    "            words = self.preprocess(document)\n",
    "            self.document_lengths.append(len(words))\n",
    "            for word in words:\n",
    "                self.index[word].append(i)\n",
    "\n",
    "        # Compute IDF for each term\n",
    "        for term in self.index:\n",
    "            self.idf[term] = 1 + np.log(self.total_documents / len(self.index[term]))\n",
    "\n",
    "    def search(self, query, top_k=10):\n",
    "        \"\"\"\n",
    "        Search the documents for the given query and return the top_k most relevant documents.\n",
    "        \"\"\"\n",
    "        query_words = self.preprocess(query)\n",
    "\n",
    "        # Compute the TF-IDF score for each document\n",
    "        scores = defaultdict(float)\n",
    "        for word in query_words:\n",
    "            if word in self.index:\n",
    "                for doc_id in self.index[word]:\n",
    "                    tf = self.documents[doc_id].count(word) / self.document_lengths[doc_id]\n",
    "                    scores[doc_id] += tf * self.idf[word]\n",
    "\n",
    "        # Sort the documents by their scores\n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the top_k most relevant documents\n",
    "        top_docs = [self.documents[doc_id] for doc_id, score in sorted_docs[:top_k]]\n",
    "\n",
    "        return top_docs\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SearchEngine(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Cow | أَيَّاماً مَّعْدُودَاتٍ فَمَن كَانَ مِنكُم مَّرِيضاً أَوْ عَلَىٰ سَفَرٍ فَعِدَّةٌ مِّنْ أَيَّامٍ أُخَرَ وَعَلَى ٱلَّذِينَ يُطِيقُونَهُ فِدْيَةٌ طَعَامُ مِسْكِينٍ فَمَن تَطَوَّعَ خَيْراً فَهُوَ خَيْرٌ لَّهُ وَأَن تَصُومُواْ خَيْرٌ لَّكُمْ إِن كُنْتُمْ تَعْلَمُونَ | 2 | 184 | Al-Baqarah | ٱلْبَقَرَة | al-Baq̈arah | Madinah | (These are) a fixed number of days. So, whoever amongst you is ill or on a journey, then he shall complete fasting for the fixed number by (fasting on) other days. But those who are not able to fast, it is obligatory on them to provide food for a needy person in lieu of that. But whoever does (greater) good seeking pleasure, that is better for him. And your fasting is better for you if you understand. + for days numbered; and if any of you be sick, or if he be on a journey, then a number of other days; and for those who are able to fast, a redemption by feeding a poor man. Yet better it is for him who volunteers good, and that you should fast is better for you, if you but know; + (Fast) a certain number of days; and (for) him who is sick among you, or on a journey, (the same) number of other days; and for those who can afford it there is a ransom: the feeding of a man in need - but whoso doeth good of his own accord, it is better for him: and that ye fast is better for you if ye did but know - +  | For days ayyāman ‘days’ is in the accusative as the object of al-siyām ‘the fast’ or of an implied yasūmū ‘he fasts’ numbered few or specific in number that is those of Ramadān as will be mentioned below; God has specified a small number as a way of facilitating matters for those under the obligation; and if any of you during the month be sick or be on a journey in which prayers are shortened or if one is strained by the fast in both cases and breaks it then a number of other days equal to the ones during which he broke his fast — let him fast them instead; and for those who are not able to do it to fast on account of old age or chronic illness a redemption which is the feeding of a poor man with about the same amount one consumes in a given day that is one mudd measure of the principal food of that town each day a variant reading has genitive fidyatin as an explicative clause. It is also said that the lā negation of the verb yutīqūnahu is not actually implied because at the very beginning of Islam they could choose between fasting or offering the redemption; but later on this was abrogated by fixing the Fast as an obligation where God says So let those of you who are present at the month fast it Q. 2185 Ibn ‘Abbās said by way of qualification ‘Except for the pregnant one and the one breastfeeding if they break their fast out of concern for the child; in the case of these two the verse remains valid and has not been abrogated’. For him who volunteers good by offering more than the minimum amount mentioned for the redemption; that volunteering is good for him; but that you should fast wa-an tasūmū is the subject is better for you khayrun lakum is its predicate than breaking the fast and paying the redemption if you but knew that this is better for you then do it. + ((Fast) a certain number of days) 30 days; (and (for) him who is sick among you, or on a journey, (the same) number of other days) let him fast the same number of days he missed; (and for those who can afford it) afford to fast (there is a ransom: the feeding of a man in need-) let him feed a person in need, instead of every day of the fast that he missed, half a measure of wheat. But this is abrogated by Allah's saying: (And whoever of you is present, let him fast the month). It is also said that this means: those who can afford a ransom and are not able to fast, old men and women who are unable to fast, they should feed a needy person, for every missed day of fast in Ramadan, half a measure of wheat. (But whoso doeth good of his own accord) gives more than two pounds, (it is better for him) in terms of reward: (and that ye fast is better for you) than giving ransom (if ye did but know-)\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.search(\"fast\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "def preprocess(document, stop_words):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stop words, and stem the words in the document.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(document.lower())\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def build_index(documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index of the preprocessed documents.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    document_lengths = []\n",
    "    for i, document in enumerate(documents):\n",
    "        words = preprocess(document, stop_words)\n",
    "        document_lengths.append(len(words))\n",
    "        for word in words:\n",
    "            index[word].append(i)\n",
    "\n",
    "    # Compute IDF for each term\n",
    "    idf = {}\n",
    "    for term in index:\n",
    "        idf[term] = 1 + np.log(len(documents) / len(index[term]))\n",
    "\n",
    "    return index, document_lengths, idf\n",
    "\n",
    "def search(query, documents, index, document_lengths, idf, top_k=10, stop_words=set(stopwords.words('english'))):\n",
    "    \"\"\"\n",
    "    Search the documents for the given query and return the top_k most relevant documents.\n",
    "    \"\"\"\n",
    "    query_words = preprocess(query, stop_words)\n",
    "\n",
    "    # Compute the TF-IDF score for each document\n",
    "    scores = defaultdict(float)\n",
    "    for word in query_words:\n",
    "        if word in index:\n",
    "            for doc_id in index[word]:\n",
    "                tf = documents[doc_id].count(word) / document_lengths[doc_id]\n",
    "                scores[doc_id] += tf * idf[word]\n",
    "\n",
    "    # Sort the documents by their scores\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top_k most relevant documents\n",
    "    top_docs = [documents[doc_id] for doc_id, score in sorted_docs[:top_k]]\n",
    "\n",
    "    return top_docs\n",
    "\n",
    "def save_model(model_path, documents):\n",
    "    index, document_lengths, idf = build_index(documents)\n",
    "    with open(model_path, 'wb') as f:\n",
    "        model = {\n",
    "            'index': index,\n",
    "            'document_lengths': document_lengths,\n",
    "            'idf': idf,\n",
    "            'stop_words': set(stopwords.words('english')),\n",
    "            'documents': documents,\n",
    "            'translation_col': translation_col,\n",
    "            'tafaseer_col': tafaseer_col,\n",
    "        }\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model['index'], model['document_lengths'], model['idf'], model['stop_words'], model['documents'], model['translation_col'], model['tafaseer_col']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/main_df.csv')\n",
    "\n",
    "documents=[]\n",
    "translation_col = list(df.columns)\n",
    "translation_col = [x for x in translation_col if \"Translation\" in x]\n",
    "\n",
    "tafaseer_col = list(df.columns)\n",
    "tafaseer_col = [x for x in tafaseer_col if \"Tafaseer\" in x]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    t = \"\"\n",
    "    t += row['Name'] + \" | \" + str(row['Arabic'])+\" | \"+ str(row['Surah'])+\" | \"+str(row['Ayat'])+\" | \"\n",
    "    t += row['EnglishTitle'] + \" | \" + str(row['ArabicTitle'])+\" | \"+ str(row['RomanTitle'])+\" | \"\n",
    "    t += row['PlaceOfRevelation'] + \" | \"\n",
    "    for j in translation_col:\n",
    "        t += row[j] + \" + \"\n",
    "        t += \" | \"\n",
    "    for j in tafaseer_col:\n",
    "        t+= row[j] + \" + \"\n",
    "        t = t[:-3]\n",
    "    documents.append(t)\n",
    "\n",
    "total_documents = len(documents)\n",
    "\n",
    "# Preprocess the documents and build the search index\n",
    "save_model(\"search_model/search_engine_model.pkl\", documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, document_lengths, idf, stop_words, documents, translation_col, tafaseer_col = load_model(\"search_model/search_engine_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Cow | أَيَّاماً مَّعْدُودَاتٍ فَمَن كَانَ مِنكُم مَّرِيضاً أَوْ عَلَىٰ سَفَرٍ فَعِدَّةٌ مِّنْ أَيَّامٍ أُخَرَ وَعَلَى ٱلَّذِينَ يُطِيقُونَهُ فِدْيَةٌ طَعَامُ مِسْكِينٍ فَمَن تَطَوَّعَ خَيْراً فَهُوَ خَيْرٌ لَّهُ وَأَن تَصُومُواْ خَيْرٌ لَّكُمْ إِن كُنْتُمْ تَعْلَمُونَ | 2 | 184 | Al-Baqarah | ٱلْبَقَرَة | al-Baq̈arah | Madinah | (These are) a fixed number of days. So, whoever amongst you is ill or on a journey, then he shall complete fasting for the fixed number by (fasting on) other days. But those who are not able to fast, it is obligatory on them to provide food for a needy person in lieu of that. But whoever does (greater) good seeking pleasure, that is better for him. And your fasting is better for you if you understand. +  | for days numbered; and if any of you be sick, or if he be on a journey, then a number of other days; and for those who are able to fast, a redemption by feeding a poor man. Yet better it is for him who volunteers good, and that you should fast is better for you, if you but know; +  | (Fast) a certain number of days; and (for) him who is sick among you, or on a journey, (the same) number of other days; and for those who can afford it there is a ransom: the feeding of a man in need - but whoso doeth good of his own accord, it is better for him: and that ye fast is better for you if ye did but know - +  | For days ayyāman ‘days’ is in the accusative as the object of al-siyām ‘the fast’ or of an implied yasūmū ‘he fasts’ numbered few or specific in number that is those of Ramadān as will be mentioned below; God has specified a small number as a way of facilitating matters for those under the obligation; and if any of you during the month be sick or be on a journey in which prayers are shortened or if one is strained by the fast in both cases and breaks it then a number of other days equal to the ones during which he broke his fast — let him fast them instead; and for those who are not able to do it to fast on account of old age or chronic illness a redemption which is the feeding of a poor man with about the same amount one consumes in a given day that is one mudd measure of the principal food of that town each day a variant reading has genitive fidyatin as an explicative clause. It is also said that the lā negation of the verb yutīqūnahu is not actually implied because at the very beginning of Islam they could choose between fasting or offering the redemption; but later on this was abrogated by fixing the Fast as an obligation where God says So let those of you who are present at the month fast it Q. 2185 Ibn ‘Abbās said by way of qualification ‘Except for the pregnant one and the one breastfeeding if they break their fast out of concern for the child; in the case of these two the verse remains valid and has not been abrogated’. For him who volunteers good by offering more than the minimum amount mentioned for the redemption; that volunteering is good for him; but that you should fast wa-an tasūmū is the subject is better for you khayrun lakum is its predicate than breaking the fast and paying the redemption if you but knew that this is better for you then do it.((Fast) a certain number of days) 30 days; (and (for) him who is sick among you, or on a journey, (the same) number of other days) let him fast the same number of days he missed; (and for those who can afford it) afford to fast (there is a ransom: the feeding of a man in need-) let him feed a person in need, instead of every day of the fast that he missed, half a measure of wheat. But this is abrogated by Allah's saying: (And whoever of you is present, let him fast the month). It is also said that this means: those who can afford a ransom and are not able to fast, old men and women who are unable to fast, they should feed a needy person, for every missed day of fast in Ramadan, half a measure of wheat. (But whoso doeth good of his own accord) gives more than two pounds, (it is better for him) in terms of reward: (and that ye fast is better for you) than giving ransom (if ye did but know-)\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"fast\", documents, index, document_lengths, idf, top_k=1, stop_words=stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.save_model('./search_model/search_engine_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_search_engine = load_model('./search_model/search_engine_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Cow | أَيَّاماً مَّعْدُودَاتٍ فَمَن كَانَ مِنكُم مَّرِيضاً أَوْ عَلَىٰ سَفَرٍ فَعِدَّةٌ مِّنْ أَيَّامٍ أُخَرَ وَعَلَى ٱلَّذِينَ يُطِيقُونَهُ فِدْيَةٌ طَعَامُ مِسْكِينٍ فَمَن تَطَوَّعَ خَيْراً فَهُوَ خَيْرٌ لَّهُ وَأَن تَصُومُواْ خَيْرٌ لَّكُمْ إِن كُنْتُمْ تَعْلَمُونَ | 2 | 184 | Al-Baqarah | ٱلْبَقَرَة | al-Baq̈arah | Madinah | (These are) a fixed number of days. So, whoever amongst you is ill or on a journey, then he shall complete fasting for the fixed number by (fasting on) other days. But those who are not able to fast, it is obligatory on them to provide food for a needy person in lieu of that. But whoever does (greater) good seeking pleasure, that is better for him. And your fasting is better for you if you understand. + for days numbered; and if any of you be sick, or if he be on a journey, then a number of other days; and for those who are able to fast, a redemption by feeding a poor man. Yet better it is for him who volunteers good, and that you should fast is better for you, if you but know; + (Fast) a certain number of days; and (for) him who is sick among you, or on a journey, (the same) number of other days; and for those who can afford it there is a ransom: the feeding of a man in need - but whoso doeth good of his own accord, it is better for him: and that ye fast is better for you if ye did but know - +  | For days ayyāman ‘days’ is in the accusative as the object of al-siyām ‘the fast’ or of an implied yasūmū ‘he fasts’ numbered few or specific in number that is those of Ramadān as will be mentioned below; God has specified a small number as a way of facilitating matters for those under the obligation; and if any of you during the month be sick or be on a journey in which prayers are shortened or if one is strained by the fast in both cases and breaks it then a number of other days equal to the ones during which he broke his fast — let him fast them instead; and for those who are not able to do it to fast on account of old age or chronic illness a redemption which is the feeding of a poor man with about the same amount one consumes in a given day that is one mudd measure of the principal food of that town each day a variant reading has genitive fidyatin as an explicative clause. It is also said that the lā negation of the verb yutīqūnahu is not actually implied because at the very beginning of Islam they could choose between fasting or offering the redemption; but later on this was abrogated by fixing the Fast as an obligation where God says So let those of you who are present at the month fast it Q. 2185 Ibn ‘Abbās said by way of qualification ‘Except for the pregnant one and the one breastfeeding if they break their fast out of concern for the child; in the case of these two the verse remains valid and has not been abrogated’. For him who volunteers good by offering more than the minimum amount mentioned for the redemption; that volunteering is good for him; but that you should fast wa-an tasūmū is the subject is better for you khayrun lakum is its predicate than breaking the fast and paying the redemption if you but knew that this is better for you then do it. + ((Fast) a certain number of days) 30 days; (and (for) him who is sick among you, or on a journey, (the same) number of other days) let him fast the same number of days he missed; (and for those who can afford it) afford to fast (there is a ransom: the feeding of a man in need-) let him feed a person in need, instead of every day of the fast that he missed, half a measure of wheat. But this is abrogated by Allah's saying: (And whoever of you is present, let him fast the month). It is also said that this means: those who can afford a ransom and are not able to fast, old men and women who are unable to fast, they should feed a needy person, for every missed day of fast in Ramadan, half a measure of wheat. (But whoso doeth good of his own accord) gives more than two pounds, (it is better for him) in terms of reward: (and that ye fast is better for you) than giving ransom (if ye did but know-)\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_search_engine.search(\"fast\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c31f30d1464bbf85f7b0336ff2375f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load the pre-trained LLM model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/CodeGPT-small-py-adaptedGPT2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/CodeGPT-small-py-adaptedGPT2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Function to preprocess the input text\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(text):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    463\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/modeling_utils.py:2208\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   2196\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2197\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2206\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   2207\u001b[0m     )\n\u001b[0;32m-> 2208\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[1;32m   2213\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/huggingface_hub/file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m   1240\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, blob_path)\n\u001b[1;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[38;5;241m.\u001b[39mname, blob_path)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/huggingface_hub/file_download.py:495\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    486\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    487\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m    488\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    489\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[1;32m    494\u001b[0m )\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    497\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "# Load the pre-trained LLM model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/CodeGPT-small-py-adaptedGPT2\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/CodeGPT-small-py-adaptedGPT2\",\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "# Function to preprocess the input text\n",
    "def preprocess(text):\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "# Function to encode the input text using the pre-trained LLM model\n",
    "def encode(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.pooler_output.detach().numpy()\n",
    "\n",
    "# Function to calculate the similarity score between the query and documents\n",
    "def get_similarities(query, documents):\n",
    "    query_vec = encode(preprocess(query))\n",
    "    doc_vecs = np.vstack([encode(preprocess(doc)) for doc in documents])\n",
    "    similarities = cosine_similarity(query_vec, doc_vecs).flatten()\n",
    "    return similarities\n",
    "\n",
    "# Function to perform search on a list of documents\n",
    "class SearchEngineLLM:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "    def search(self, query, top_k=10):\n",
    "        similarities = get_similarities(query, self.documents)\n",
    "        indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [(self.documents[i], similarities[i]) for i in indices]\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Train the search engine on a list of documents\n",
    "    documents = [\n",
    "        \"This is a document about Python programming\",\n",
    "        \"Java is another programming language\",\n",
    "        \"Web development involves building websites\",\n",
    "        \"Machine learning is a field of study that involves building predictive models\",\n",
    "        \"Software engineering is the process of building software systems\",\n",
    "        \"Data science involves analyzing data to extract insights\",\n",
    "        \"Artificial intelligence is the development of intelligent machines\",\n",
    "        \"Computer vision is a field of study that involves teaching computers to interpret visual data\",\n",
    "        \"Natural language processing is the development of algorithms for analyzing and understanding human languages\",\n",
    "        \"Robotics is the design, construction, and operation of robots\",\n",
    "        \"The internet is a global network of interconnected computers\",\n",
    "        \"Cloud computing involves using remote servers to store, manage, and process data\",\n",
    "    ]\n",
    "    search_engine = SearchEngineLLM(documents)\n",
    "    \n",
    "    # Save the trained model using pickle\n",
    "    with open('search_engine_model_llm.pkl', 'wb') as f:\n",
    "        pickle.dump(search_engine, f)\n",
    "    \n",
    "    # Load the trained model using pickle\n",
    "    with open('search_engine_model_llm.pkl', 'rb') as f:\n",
    "        search_engine = pickle.load(f)\n",
    "    \n",
    "    # Perform search on the trained model\n",
    "    query = \"programming languages\"\n",
    "    results = search_engine.search(query, top_k=5)\n",
    "    \n",
    "    # Display the search results\n",
    "    print(f\"Top {len(results)} results for '{query}':\")\n",
    "    for i, (document, similarity) in enumerate(results):\n",
    "        print(f\"{i+1}. {document} (Similarity: {similarity:.2f})\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "# translate Urdu text to English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are Pakistanis\n",
      "my name is rashid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "urdu_text = \"ہم پاکستانی\"\n",
    "english_text = GoogleTranslator(source='auto', target='en').translate(urdu_text)\n",
    "print(english_text)  # output: \"We are Pakistani\"\n",
    "\n",
    "# translate Roman text to English\n",
    "roman_text = \"mera naam Rashid hai\"\n",
    "english_text = GoogleTranslator(source='auto', target='en').translate(roman_text)\n",
    "print(english_text)  # output: \"My name is Rashid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'afrikaans': 'af',\n",
       " 'albanian': 'sq',\n",
       " 'amharic': 'am',\n",
       " 'arabic': 'ar',\n",
       " 'armenian': 'hy',\n",
       " 'assamese': 'as',\n",
       " 'aymara': 'ay',\n",
       " 'azerbaijani': 'az',\n",
       " 'bambara': 'bm',\n",
       " 'basque': 'eu',\n",
       " 'belarusian': 'be',\n",
       " 'bengali': 'bn',\n",
       " 'bhojpuri': 'bho',\n",
       " 'bosnian': 'bs',\n",
       " 'bulgarian': 'bg',\n",
       " 'catalan': 'ca',\n",
       " 'cebuano': 'ceb',\n",
       " 'chichewa': 'ny',\n",
       " 'chinese (simplified)': 'zh-CN',\n",
       " 'chinese (traditional)': 'zh-TW',\n",
       " 'corsican': 'co',\n",
       " 'croatian': 'hr',\n",
       " 'czech': 'cs',\n",
       " 'danish': 'da',\n",
       " 'dhivehi': 'dv',\n",
       " 'dogri': 'doi',\n",
       " 'dutch': 'nl',\n",
       " 'english': 'en',\n",
       " 'esperanto': 'eo',\n",
       " 'estonian': 'et',\n",
       " 'ewe': 'ee',\n",
       " 'filipino': 'tl',\n",
       " 'finnish': 'fi',\n",
       " 'french': 'fr',\n",
       " 'frisian': 'fy',\n",
       " 'galician': 'gl',\n",
       " 'georgian': 'ka',\n",
       " 'german': 'de',\n",
       " 'greek': 'el',\n",
       " 'guarani': 'gn',\n",
       " 'gujarati': 'gu',\n",
       " 'haitian creole': 'ht',\n",
       " 'hausa': 'ha',\n",
       " 'hawaiian': 'haw',\n",
       " 'hebrew': 'iw',\n",
       " 'hindi': 'hi',\n",
       " 'hmong': 'hmn',\n",
       " 'hungarian': 'hu',\n",
       " 'icelandic': 'is',\n",
       " 'igbo': 'ig',\n",
       " 'ilocano': 'ilo',\n",
       " 'indonesian': 'id',\n",
       " 'irish': 'ga',\n",
       " 'italian': 'it',\n",
       " 'japanese': 'ja',\n",
       " 'javanese': 'jw',\n",
       " 'kannada': 'kn',\n",
       " 'kazakh': 'kk',\n",
       " 'khmer': 'km',\n",
       " 'kinyarwanda': 'rw',\n",
       " 'konkani': 'gom',\n",
       " 'korean': 'ko',\n",
       " 'krio': 'kri',\n",
       " 'kurdish (kurmanji)': 'ku',\n",
       " 'kurdish (sorani)': 'ckb',\n",
       " 'kyrgyz': 'ky',\n",
       " 'lao': 'lo',\n",
       " 'latin': 'la',\n",
       " 'latvian': 'lv',\n",
       " 'lingala': 'ln',\n",
       " 'lithuanian': 'lt',\n",
       " 'luganda': 'lg',\n",
       " 'luxembourgish': 'lb',\n",
       " 'macedonian': 'mk',\n",
       " 'maithili': 'mai',\n",
       " 'malagasy': 'mg',\n",
       " 'malay': 'ms',\n",
       " 'malayalam': 'ml',\n",
       " 'maltese': 'mt',\n",
       " 'maori': 'mi',\n",
       " 'marathi': 'mr',\n",
       " 'meiteilon (manipuri)': 'mni-Mtei',\n",
       " 'mizo': 'lus',\n",
       " 'mongolian': 'mn',\n",
       " 'myanmar': 'my',\n",
       " 'nepali': 'ne',\n",
       " 'norwegian': 'no',\n",
       " 'odia (oriya)': 'or',\n",
       " 'oromo': 'om',\n",
       " 'pashto': 'ps',\n",
       " 'persian': 'fa',\n",
       " 'polish': 'pl',\n",
       " 'portuguese': 'pt',\n",
       " 'punjabi': 'pa',\n",
       " 'quechua': 'qu',\n",
       " 'romanian': 'ro',\n",
       " 'russian': 'ru',\n",
       " 'samoan': 'sm',\n",
       " 'sanskrit': 'sa',\n",
       " 'scots gaelic': 'gd',\n",
       " 'sepedi': 'nso',\n",
       " 'serbian': 'sr',\n",
       " 'sesotho': 'st',\n",
       " 'shona': 'sn',\n",
       " 'sindhi': 'sd',\n",
       " 'sinhala': 'si',\n",
       " 'slovak': 'sk',\n",
       " 'slovenian': 'sl',\n",
       " 'somali': 'so',\n",
       " 'spanish': 'es',\n",
       " 'sundanese': 'su',\n",
       " 'swahili': 'sw',\n",
       " 'swedish': 'sv',\n",
       " 'tajik': 'tg',\n",
       " 'tamil': 'ta',\n",
       " 'tatar': 'tt',\n",
       " 'telugu': 'te',\n",
       " 'thai': 'th',\n",
       " 'tigrinya': 'ti',\n",
       " 'tsonga': 'ts',\n",
       " 'turkish': 'tr',\n",
       " 'turkmen': 'tk',\n",
       " 'twi': 'ak',\n",
       " 'ukrainian': 'uk',\n",
       " 'urdu': 'ur',\n",
       " 'uyghur': 'ug',\n",
       " 'uzbek': 'uz',\n",
       " 'vietnamese': 'vi',\n",
       " 'welsh': 'cy',\n",
       " 'xhosa': 'xh',\n",
       " 'yiddish': 'yi',\n",
       " 'yoruba': 'yo',\n",
       " 'zulu': 'zu'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoogleTranslator().get_supported_languages(as_dict=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"user@doma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(email)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda email: re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', email) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
